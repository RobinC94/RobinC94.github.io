---
layout: post
title:  "Decoupled neural interfaces"
date:   2019-12-03
excerpt: "Decoupled neural interfaces using synthetic gradients."
tag:
- paper
- training method
- rnn
- google
---

[Paper link](https://arxiv.org/pdf/1608.05343.pdf)
综述： 
 训练方法的创新，提出构建一个子模型来预测 DNN 模型每一层的更新值，用以解决 RNN 或分布式 CNN 中的训练问题

简介：
 DNN 模型的训练是通过反向传播完成的，一轮参数的更新需要 DNN 完成一次完整的前向传播（计算 loss）和反向传播，由此引入文章当中的 lock 概念，即 Forward Locking，Update Locking 和 Backwards Locking。文章提出构建一个专门预测反向传播的 gradient 或其他类型值的模型 Decoupled neural interfaces（DNI）以 DNN 每一层的输出 map 或 activation，加上 DNI 的参数以及可能包含 label 信息共同预测当前层的 gradients，以实现各层的异步更新。该方法主要对 RNN 意义较大，可以解除 RNN 训练时需计算后面有限步时间序列的结果的缺点，可以训练 DNI 以更好地预测无限时间后的 RNN 反向传播的梯度结果。DNI 可同时预测其他类型的训练返回结果，如强化学习中与 score 有关的 value。文章同时提出 synthetic gradients 可与 real gradients 共同使用

细节：
* 采用 L2 loss 学习实际梯度与预测梯度
* 

